\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

\geometry{a4paper, margin=1in}

\title{Text Classification Assignment: Sport vs. Politics}
\author{Ronak Gadhiya (B22AI052)}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This report documents the design, implementation, and evaluation of a text classification system capable of distinguishing between "Sport" and "Politics" news articles. The system utilizes the BBC News dataset and compares three machine learning algorithms: Naive Bayes, Support Vector Machines (SVM), and Logistic Regression, across three feature representation techniques: Bag of Words (BoW), TF-IDF, and N-Grams. The complete source code and resources for this project are available at: \url{https://github.com/ronakgadhiya09/NLU-Assignment1}.

\section{Data Collection and Description}
\subsection{Data Source}
The dataset used for this assignment is the \textbf{BBC News Dataset}, a standard benchmark for text classification. It consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.

\subsection{Data Preparation and Cleaning}
For this specific task, we filtered the dataset to include only two categories:
\begin{itemize}
    \item \textbf{Sport}
    \item \textbf{Politics}
\end{itemize}
This results in a binary classification problem.

\textbf{Preprocessing steps included:}
\begin{enumerate}
    \item \textbf{Loading}: Reading raw text files from the dataset directories.
    \item \textbf{Filtering}: Explicitly selecting only folders named 'sport' and 'politics'.
    \item \textbf{Splitting}: The data was split into training (80\%) and testing (20\%) sets using stratified sampling to maintain class distribution.
    \item \textbf{Vectorization}: Converting raw text into numerical features using Scikit-Learn's vectorizers (CountVectorizer, TfidfVectorizer). Standard English stop words were removed during this process.
\end{enumerate}

\subsection{Dataset Statistics}
The filtered dataset consists of a total of 928 documents, distributed as follows:
\begin{itemize}
    \item \textbf{Sport}: 511 documents
    \item \textbf{Politics}: 417 documents
\end{itemize}

\subsection{Sample Data Points}
Below are snippet examples from each category:

\textbf{Sport}:
\begin{quote}
\textit{Claxton hunting first major medal}\\
British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid.
\end{quote}

\textbf{Politics}:
\begin{quote}
\textit{Labour plans maternity pay rise}\\
Maternity pay for new mothers is to rise by \pounds1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt.
\end{quote}

\section{Techniques and Methodologies}

\subsection{Feature Representation}
We explored three distinct methods to represent text data as numerical vectors:

\begin{enumerate}
    \item \textbf{Bag of Words (BoW)}: Represents text as a collection of its words, disregarding grammar and word order but keeping multiplicity.
    \item \textbf{TF-IDF (Term Frequency-Inverse Document Frequency)}: Evaluates how relevant a word is to a document in a collection. It increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.
    \item \textbf{N-Grams (Bi-grams)}: Captures local context by considering sequences of $N$ contiguous items (words). We used Bi-grams ($N=2$).
\end{enumerate}

\subsection{Machine Learning Classifiers}
We implemented and compared three classifiers:

\begin{enumerate}
    \item \textbf{Naive Bayes (MultinomialNB)}: A probabilistic classifier based on Bayes' theorem with an assumption of independence between features. It is particularly effective for text classification with discrete features (like word counts).
    \item \textbf{Support Vector Machine (SVM)}: Finds the hyperplane that best separates the classes in a high-dimensional space. We used a \textbf{Linear Kernel}, which is computationally efficient and often performs best for high-dimensional text data.
    \item \textbf{Logistic Regression}: A statistical model that uses a logistic function to model a binary dependent variable. It provides probabilities for class membership and is a robust baseline for binary classification.
\end{enumerate}

\section{Quantitative Analysis and Comparison}

\subsection{Performance Metrics}
We evaluated the models using \textbf{Accuracy}, \textbf{Precision}, \textbf{Recall}, and \textbf{F1-Score}. The results on the test set are summarized below:

\begin{table}[H]
    \centering
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Model} & \textbf{Feature} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
        \midrule
        Naive Bayes & BoW & \textbf{1.0000} & 1.0000 & 1.0000 & 1.0000 \\
        SVM & BoW & 0.9892 & 0.9895 & 0.9892 & 0.9892 \\
        Logistic Regression & BoW & 0.9892 & 0.9895 & 0.9892 & 0.9892 \\
        \midrule
        Naive Bayes & TF-IDF & \textbf{1.0000} & 1.0000 & 1.0000 & 1.0000 \\
        SVM & TF-IDF & \textbf{1.0000} & 1.0000 & 1.0000 & 1.0000 \\
        Logistic Regression & TF-IDF & 0.9946 & 0.9947 & 0.9946 & 0.9946 \\
        \midrule
        Naive Bayes & N-Grams & \textbf{1.0000} & 1.0000 & 1.0000 & 1.0000 \\
        SVM & N-Grams & 0.9355 & 0.9423 & 0.9355 & 0.9348 \\
        Logistic Regression & N-Grams & 0.9355 & 0.9423 & 0.9355 & 0.9348 \\
        \bottomrule
    \end{tabular}
    \caption{Model Performance Comparison}
    \label{tab:results}
\end{table}

\subsection{Confusion Matrices}
Below are the confusion matrices for the best performing configurations.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{results/cm_Naive_Bayes_TF-IDF.png}
        \caption{Naive Bayes (TF-IDF)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{results/cm_SVM_TF-IDF.png}
        \caption{SVM (TF-IDF)}
    \end{subfigure}
    \caption{Confusion Matrices for Top Models}
\end{figure}

\subsection{Observations}
\begin{itemize}
    \item \textbf{Best Performer}: \textbf{Naive Bayes} achieved perfect classification (100\% accuracy) across all feature sets (BoW, TF-IDF, N-Grams). This is indicative of the clear separability of the "Sport" and "Politics" vocabularies.
    \item \textbf{SVM and TF-IDF}: SVM with TF-IDF also achieved 100\% accuracy. The high dimension of TF-IDF features is well-handled by the Linear SVM.
    \item \textbf{N-Grams Issue}: While Naive Bayes handled N-Grams perfectly, both SVM and Logistic Regression saw a drop in performance (~93.5\%). This is likely due to the \textbf{curse of dimensionality}; bi-grams drastically increase the feature space size.
    \item \textbf{Feature Impact}: TF-IDF generally offered the most stable high performance across models.
\end{itemize}

\section{Limitations of the System}
\begin{enumerate}
    \item \textbf{Dataset Bias}: The model is trained on BBC news from 2004-2005. It may not generalize well to modern news or different writing styles.
    \item \textbf{Out-of-Vocabulary (OOV) words}: The models can only handle words seen during training. New, unseen words in test data are ignored.
    \item \textbf{Context Sensitivity}: While N-grams capture local context, deeper semantic meaning is largely lost compared to modern Deep Learning approaches.
    \item \textbf{Static Data}: The model does not learn incrementally; it requires retraining to incorporate new data.
\end{enumerate}

\section{Conclusion}
The constructed system successfully classifies Sport vs. Politics articles with high accuracy. The comparison highlights that while simple methods like Naive Bayes are fast and effective, discriminative models like SVM often provide superior performance for high-dimensional text data.

\end{document}
